{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noble-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import newsgroup dataset from sklearn datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# select the categories of interest\n",
    "categories = ['talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'alt.atheism', 'soc.religion.christian', 'sci.space', 'sci.med', 'talk.religion.misc']\n",
    "\n",
    "# sample a training set\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-fireplace",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "generous-temperature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 alt.atheism\n",
      "i: 1 sci.med\n",
      "i: 2 sci.space\n",
      "i: 3 soc.religion.christian\n",
      "i: 4 talk.politics.guns\n",
      "i: 5 talk.politics.mideast\n",
      "i: 6 talk.politics.misc\n",
      "i: 7 talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(categories)):\n",
    "    print(\"i: \" + str(i) + \" \" + twenty_train.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complicated-keeping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4218x56634 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 794945 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reduced-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian (Probability: 0.83)\n",
      "\"blabla I don't belong to anything\" => talk.politics.mideast (Probability: 0.22)\n",
      "'We will explore Mars' => sci.space (Probability: 0.64)\n",
      "'Human DNA for Genome' => sci.med (Probability: 0.45)\n",
      "'Israel and Palestine are at War' => talk.politics.mideast (Probability: 1.00)\n",
      "'Authorize gun dealers' => talk.politics.guns (Probability: 0.91)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We now apply the (Multinomial) Naive Bayes; \n",
    "\n",
    "# Remember the 5 steps we need to apply...\n",
    "\n",
    "# 1. Select model and import it\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 2. Select model hyperparameters; alpha is a smoothing parameter\n",
    "model = MultinomialNB(alpha=10)\n",
    "\n",
    "# 3. Arrange data in feature matrix / perform feature engineering\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "Y = twenty_train.target\n",
    "\n",
    "# 4. Fit model to data\n",
    "clf = model.fit(X_train_counts, Y)\n",
    "\n",
    "# 5. Apply model to new examples\n",
    "docs_new = ['God is love', 'blabla I don\\'t belong to anything' ,'We will explore Mars','Human DNA for Genome', 'Israel and Palestine are at War', 'Authorize gun dealers']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "\n",
    "#Take in account probability as in the speeches, many sentences will not belong to any of the categories\n",
    "predicted_proba = clf.predict_proba(X_new_counts)\n",
    "\n",
    "for doc, probasArray in zip(docs_new, predicted_proba):\n",
    "    indexMaxProbas = np.argmax(probasArray)\n",
    "    print('%r => %s (Probability: %.2f)' % (doc, twenty_train.target_names[indexMaxProbas], probasArray[indexMaxProbas]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cellular-science",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\nfrom pprint import pprint\\nfrom sklearn.model_selection import GridSearchCV\\nimport numpy as np\\nfrom time import time\\nimport logging\\n\\npipeline = Pipeline([\\n    (\\'vect\\', CountVectorizer()),\\n    (\\'tfidf\\', TfidfTransformer()),\\n    (\\'clf\\', MultinomialNB())])\\n\\nparameters = {\\n    \\'vect__ngram_range\\': ((1, 1), (1, 2), (1,3), (1,4)),  # unigrams or bigrams\\n    \\'tfidf__use_idf\\': (True, False),\\n    \\'clf__alpha\\': (0.1, 0.01, 0.001, 0.0001, 1),\\n}\\n\\nif __name__ == \"__main__\":\\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\\n\\n    print(\"Performing grid search...\")\\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\\n    print(\"parameters:\")\\n    print(parameters)\\n    t0 = time()\\n    grid_search.fit(twenty_train.data, twenty_train.target)\\n    print(\"done in %0.3fs\" % (time() - t0))\\n    print()\\n\\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\\n    print(\"Best parameters set:\")\\n    best_parameters = grid_search.best_estimator_.get_params()\\n    for param_name in sorted(parameters.keys()):\\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1,3), (1,4)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (0.1, 0.01, 0.001, 0.0001, 1),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(twenty_train.data, twenty_train.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tested-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[259   2   2  13   1   0   2  15]\n",
      " [  2 338   5   7   0   3   8   3]\n",
      " [  0   9 361   0   1   0   3   0]\n",
      " [  2   2   1 376   1   0   0   6]\n",
      " [  0   2   1   0 329   0   9   7]\n",
      " [  3   0   0   1   1 346  11   1]\n",
      " [  2   3   7   1  50   1 208  11]\n",
      " [ 33   2   5  19   7   1   3 158]]\n",
      "0.8982602118003026\n",
      "\n",
      " \n",
      " ----------------------------- \n",
      " \n",
      "\n",
      "[[264   3   3  18   1   2   3  25]\n",
      " [  3 348   8  12   5   5  10   5]\n",
      " [  1  10 369   3   4   1   6   0]\n",
      " [  3   5   1 380   1   0   2   6]\n",
      " [  1   5   2   0 334   0  13   9]\n",
      " [  3   1   0   2   3 352  14   1]\n",
      " [  2   8   7   1  60   1 217  14]\n",
      " [ 40   4   6  23   9   1   4 164]]\n",
      "0.8646723646723646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', MultinomialNB(alpha=0.001))])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# create a test set -> this is a set different than the train set\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted_probas = text_clf.predict_proba(docs_test)\n",
    "\n",
    "good_pred = np.array([])\n",
    "good_target = np.array([])\n",
    "for probasArray, target in zip(predicted_probas, twenty_test.target):\n",
    "    indexMaxProbas = np.argmax(probasArray)\n",
    "    if (probasArray[indexMaxProbas] > 0.70):\n",
    "        good_pred = np.append(good_pred, indexMaxProbas)\n",
    "        good_target = np.append(good_target, target)\n",
    "\n",
    "good_pred =  good_pred.astype(int)\n",
    "good_target =  good_target.astype(int)\n",
    "matOptiFiltered = confusion_matrix(good_target, good_pred)\n",
    "\n",
    "print(matOptiFiltered)\n",
    "print(np.mean(good_target == good_pred))\n",
    "predicted = text_clf.predict(docs_test)\n",
    "matOptiNonFiltered = confusion_matrix(twenty_test.target, predicted)\n",
    "print(\"\\n \\n ----------------------------- \\n \\n\")\n",
    "print(matOptiNonFiltered)\n",
    "print(np.mean(predicted == twenty_test.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controlled-melbourne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO-alpha3 Code</th>\n",
       "      <th>Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>WSM</td>\n",
       "      <td>Mr. President,\\nDistinguished delegates,\\nLadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8477</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>YEM</td>\n",
       "      <td>In the name of God the Merciful and the Compas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>ZAF</td>\n",
       "      <td>President of the General Assembly, Secretary-G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8479</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>Your excellency Mr. Volkan Bozkir, President o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8480</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Your Excellency, Ambassador Volkan Bozkir, Pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Session  Year ISO-alpha3 Code  \\\n",
       "8476       75  2020             WSM   \n",
       "8477       75  2020             YEM   \n",
       "8478       75  2020             ZAF   \n",
       "8479       75  2020             ZMB   \n",
       "8480       75  2020             ZWE   \n",
       "\n",
       "                                                 Speech  \n",
       "8476  Mr. President,\\nDistinguished delegates,\\nLadi...  \n",
       "8477  In the name of God the Merciful and the Compas...  \n",
       "8478  President of the General Assembly, Secretary-G...  \n",
       "8479  Your excellency Mr. Volkan Bozkir, President o...  \n",
       "8480  Your Excellency, Ambassador Volkan Bozkir, Pre...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sessions = np.arange(25, 76)\n",
    "data=[]\n",
    "\n",
    "for session in sessions:\n",
    "    directory = \"../data/TXT/Session \"+str(session)+\" - \"+str(1945+session)\n",
    "    for filename in os.listdir(directory):\n",
    "        f = open(os.path.join(directory, filename), encoding=\"utf8\")\n",
    "        if filename[0]==\".\": #ignore hidden files\n",
    "            continue\n",
    "        splt = filename.split(\"_\")\n",
    "        data.append([session, 1945+session, splt[0], f.read()])\n",
    "      \n",
    "df_speech = pd.DataFrame(data, columns=['Session','Year','ISO-alpha3 Code','Speech'])\n",
    "\n",
    "df_speech.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "published-opera",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 67, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-54d99921b79d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/UNSD — Methodology.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_codes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_speech\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ISO-alpha3 Code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Country or Area\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Region Name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Sub-region Name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ISO-alpha3 Code\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Developed / Developing Countries\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Session\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Year\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Speech\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_un_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ISO-alpha3 Code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_un_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2059\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 67, saw 2\n"
     ]
    }
   ],
   "source": [
    "df_codes = pd.read_csv('../data/UNSD — Methodology.csv')\n",
    "df_merged = pd.merge(df_codes, df_speech, on='ISO-alpha3 Code')\n",
    "df_merged = df_merged[[\"Country or Area\", \"Region Name\",\"Sub-region Name\", \"ISO-alpha3 Code\",\"Developed / Developing Countries\", \"Session\", \"Year\", \"Speech\"]]\n",
    "df_un_merged = df_merged.set_index(['Year','ISO-alpha3 Code'])\n",
    "df_un_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-buying",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sessions = np.arange(45, 76)\n",
    "\n",
    "occurencesByYearDvlpd = []\n",
    "occurencesByYearDvlpng = []\n",
    "\n",
    "for session in sessions:\n",
    "    speeches_developed = []\n",
    "    speeches_developing = []\n",
    "    year = 1945 + session\n",
    "    for i in range(len(df_un_merged.loc[year,:][\"Speech\"])):\n",
    "        speech = df_un_merged.loc[year,:][\"Speech\"][i]\n",
    "        isDeveloped = (df_un_merged.loc[year,:][\"Developed / Developing Countries\"][i] == \"Developed\")\n",
    "        pattern = r'[0-9]'\n",
    "        # Match all digits in the string and replace them by empty string\n",
    "        speech = re.sub(pattern, '', speech)\n",
    "        speech_splitted = speech.split('.') #split by sentences\n",
    "        speech_cleaned = [re.sub('\\W+',' ', element) for element in speech_splitted] #take out special characters\n",
    "        speech_cleaned = [x for x in speech_cleaned if x] \n",
    "        speech_cleaned = [x for x in speech_cleaned if x.strip()] #take out any empty elements\n",
    "        if (isDeveloped):\n",
    "            speeches_developed += speech_cleaned\n",
    "        else:\n",
    "            speeches_developing += speech_cleaned\n",
    "    \n",
    "\n",
    "    predicted_probas_speeches_dvlpd = text_clf.predict_proba(speeches_developed)\n",
    "    predicted_probas_speeches_dvlpng = text_clf.predict_proba(speeches_developing)\n",
    "    \n",
    "    max_row_proba_index = np.argmax(predicted_probas_speeches_dvlpd, axis=1)\n",
    "    mask_proba = np.any(predicted_probas_speeches_dvlpd>0.65, axis=1) #mask to keep result only\n",
    "                                                                #if the max proba > 0.65\n",
    "    filtered_predictions_dvlpd = max_row_proba_index[mask_proba]\n",
    "    \n",
    "    max_row_proba_index = np.argmax(predicted_probas_speeches_dvlpng, axis=1)\n",
    "    mask_proba = np.any(predicted_probas_speeches_dvlpng>0.65, axis=1) #mask to keep result only\n",
    "                                                                #if the max proba > 0.65\n",
    "    filtered_predictions_dvlpng = max_row_proba_index[mask_proba]\n",
    "\n",
    "    occurencesListDvlpd = [(filtered_predictions_dvlpd == i).sum() for i in range(8)]\n",
    "    occurencesListDvlpng = [(filtered_predictions_dvlpng == i).sum() for i in range(8)]\n",
    "    \n",
    "    occurencesByYearDvlpd.append(occurencesListDvlpd)\n",
    "    occurencesByYearDvlpng.append(occurencesListDvlpng)\n",
    "\n",
    "arrayOccurencesDvlp = np.array(occurencesByYearDvlpd)\n",
    "arrayOccurencesDvlng = np.array(occurencesByYearDvlpng)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = arrayOccurencesDvlp.shape\n",
    "arrayFrequenciesDvlp = np.ones(shape)\n",
    "arrayFrequenciesDvlpng = np.ones(shape)\n",
    "for i in range(shape[0]):\n",
    "    sumRowDvlp = arrayOccurencesDvlp[i].sum()\n",
    "    sumRowDvlpng = arrayOccurencesDvlng[i].sum()\n",
    "    if i==1:\n",
    "        print(sumRowDvlp)\n",
    "    for j in range(shape[1]):\n",
    "        if (j == 5 and i==1):\n",
    "            print(arrayOccurencesDvlp[i,j]/sumRowDvlp)\n",
    "        arrayFrequenciesDvlp[i,j] = arrayOccurencesDvlp[i,j]/sumRowDvlp\n",
    "        arrayFrequenciesDvlpng[i,j] = arrayOccurencesDvlng[i,j]/sumRowDvlpng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(sessions+1945, arrayFrequenciesDvlp[:,5], label = \"Mid East, developped\")\n",
    "plt.plot(sessions+1945, arrayFrequenciesDvlpng[:,5], label = \"Mid East, developping\")\n",
    "plt.plot([2011, 2011], [0.4, 0.6], label = \"test\")\n",
    "#plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(sessions+1945, arrayFrequenciesDvlp[:,1], label = \"Science Medecine, developped\")\n",
    "plt.plot(sessions+1945, arrayFrequenciesDvlpng[:,1], label = \"Science Medecine, developping\")\n",
    "#plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "plt.plot([2007, 2007], [0, 0.04], label = \"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sessions = np.arange(45, 76)\n",
    "\n",
    "occurenceOfMidEastByYear = []\n",
    "occurenceOfScienceByYear = []\n",
    "occurenceOfReligionByYear = []\n",
    "occurencesByYear = []\n",
    "\n",
    "for session in sessions:\n",
    "    speeches = []\n",
    "    year = 1945 + session\n",
    "    for i in range(len(df_un_merged.loc[year,:][\"Speech\"])):\n",
    "        speech = df_un_merged.loc[year,:][\"Speech\"][i]\n",
    "        pattern = r'[0-9]'\n",
    "        # Match all digits in the string and replace them by empty string\n",
    "        speech = re.sub(pattern, '', speech)\n",
    "        speech_splitted = speech.split('.') #split by sentences\n",
    "        speech_cleaned = [re.sub('\\W+',' ', element) for element in speech_splitted] #take out special characters\n",
    "        speech_cleaned = [x for x in speech_cleaned if x] \n",
    "        speech_cleaned = [x for x in speech_cleaned if x.strip()] #take out any empty elements\n",
    "        speeches += speech_cleaned\n",
    "    #X_speeches = count_vect.transform(speeches)\n",
    "    predicted_probas_speeches = text_clf.predict_proba(speeches)\n",
    "    #print(\"Number of predicted sentences: \", len(predicted_probas_speeches))\n",
    "    max_row_proba_index = np.argmax(predicted_probas_speeches, axis=1)\n",
    "    mask_proba = np.any(predicted_probas_speeches>0.55, axis=1) #mask to keep result only\n",
    "                                                                #if the max proba > 0.65\n",
    "    filtered_predictions = max_row_proba_index[mask_proba]\n",
    "    #print(\"Number of predicted sentences (cleaned): \", len(filtered_predictions))\n",
    "    \"\"\"\n",
    "    nbOccMidEast = (filtered_predictions == 5).sum()\n",
    "    nbOccScience = (filtered_predictions == 1).sum()\n",
    "    nbOccReligion = (filtered_predictions == 7).sum()\n",
    "    occurenceOfScienceByYear.append(nbOccScience)\n",
    "    occurenceOfMidEastByYear.append(nbOccMidEast)\n",
    "    occurenceOfReligionByYear.append(nbOccReligion)\n",
    "    \"\"\"\n",
    "    \n",
    "    nbOccScience = (filtered_predictions == 1).sum()\n",
    "    occurenceOfScienceByYear.append(nbOccScience)\n",
    "    occurencesList = [(filtered_predictions == i).sum() for i in range(8)]\n",
    "    occurencesByYear.append(occurencesList)\n",
    "\n",
    "arrayOccurences = np.array(occurencesByYear)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurenceOfMidEastByYear = arrayOccurences[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(sessions+1945, occurenceOfMidEastByYear, label = \"Mid East\")\n",
    "#plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "plt.plot([2001, 2001], [7000, 3500], label = \"twin towers attack\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "#plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "plt.plot([2019, 2019], [0, 500], label = \"corona attack\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(sessions+1945, occurenceOfReligionByYear, label = \"Religion\")\n",
    "#plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "plt.plot([2001, 2001], [0, 300], label = \"twin towers attack\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-shopping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "sessions = np.arange(45, 76)\n",
    "\n",
    "occurenceOfMidEastByYear = []\n",
    "occurenceOfScienceByYear = []\n",
    "occurenceOfReligionByYear = []\n",
    "for session in sessions:\n",
    "    speeches = []\n",
    "    year = 1945 + session\n",
    "    speech = df_un_merged.loc[year,\"USA\"][\"Speech\"]\n",
    "    pattern = r'[0-9]'\n",
    "    # Match all digits in the string and replace them by empty string\n",
    "    speech = re.sub(pattern, '', speech)\n",
    "    speech_splitted = speech.split('.') #split by sentences\n",
    "    speech_cleaned = [re.sub('\\W+',' ', element) for element in speech_splitted] #take out special characters\n",
    "    speech_cleaned = [x for x in speech_cleaned if x] \n",
    "    speech_cleaned = [x for x in speech_cleaned if x.strip()] #take out any empty elements\n",
    "    speeches = speeches + speech_cleaned\n",
    "    #X_speeches = count_vect.transform(speeches)\n",
    "    predicted_probas_speeches = text_clf.predict_proba(speeches)\n",
    "    #print(\"Number of predicted sentences: \", len(predicted_probas_speeches))\n",
    "    max_row_proba_index = np.argmax(predicted_probas_speeches, axis=1)\n",
    "    mask_proba = np.any(predicted_probas_speeches>0.40, axis=1) #mask to keep result only\n",
    "                                                                #if the max proba > 0.65\n",
    "    filtered_predictions = max_row_proba_index[mask_proba]\n",
    "    #print(\"Number of predicted sentences (cleaned): \", len(filtered_predictions))\n",
    "    nbOccMidEast = (filtered_predictions == 5).sum()\n",
    "    nbOccScience = (filtered_predictions == 1).sum()\n",
    "    nbOccReligion = (filtered_predictions == 7).sum()\n",
    "    occurenceOfScienceByYear.append(nbOccScience)\n",
    "    occurenceOfMidEastByYear.append(nbOccMidEast)\n",
    "    occurenceOfReligionByYear.append(nbOccReligion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(sessions+1945, occurenceOfMidEastByYear, label = \"Mid East\")\n",
    "#plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "plt.plot([2001, 2001], [0, 100], label = \"twin towers attack\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-waters",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "#plt.plot(sessions+1945, occurenceOfScienceByYear, label = \"Science Medecine\")\n",
    "plt.plot([2019, 2019], [0, 15], label = \"corona attack\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-conversion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
